{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generacion de nuestro propio modelo de W2V a partir de los datos de twitter y analisis de similitud entre usuarios a partir de estos vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lo que hacemos es utilizar una red neuronal para interrelacionar las palabras de un texto. Como?\n",
    "\n",
    "usando un autoencoder al que luego quitamos la ultima capa, esto nos da los vectores de cada palabra una red neuronal con n entradas (palabras unicas en el texto), m neuronas en capa intermedia, y o neuronas en la salida. el sistema intenta predecir qué palabra vendrá despues de cada palabra de entrada. siendo la palabra un vector de 1Xn.\n",
    "\n",
    "el sistema aprenderá que palabras con vectores parecidos estan inter relacionadas (pertenecen al mismo tema)\n",
    "\n",
    "al cortar la capa final, lo que nos queda es una matriz de datos con la probabilidad de aparicion de una palabra para las demas. eso es un vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.spatial import distance\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configLocal\n",
    "server = configLocal.server\n",
    "database = configLocal.database\n",
    "username = configLocal.username\n",
    "password = configLocal.password\n",
    "cnxn = configLocal.cnxn\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "data = pd.read_sql_query(\"select msg_id, msg_users_id, msg_timestamp, msg_text from dbo.msg\", cnxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> 7157\n"
     ]
    }
   ],
   "source": [
    "#parse database to dict\n",
    "corpus = {}\n",
    "for i in range(data.shape[0]):\n",
    "    if data.iloc[i][\"msg_users_id\"] in corpus:\n",
    "        corpus[data.iloc[i][\"msg_users_id\"]].append(re.sub(r'http\\S+', '', data.iloc[i][\"msg_text\"], flags=re.MULTILINE))\n",
    "    else:\n",
    "        corpus[data.iloc[i][\"msg_users_id\"]] = [re.sub(r'http\\S+', '', data.iloc[i][\"msg_text\"], flags=re.MULTILINE)]\n",
    "\n",
    "print(type(corpus), len(corpus))\n",
    "\n",
    "for i in corpus.keys():\n",
    "    corpus[i] = gensim.utils.simple_preprocess(\"\".join(corpus[i]).encode('utf-8'), deacc=True, min_len=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "carga del giga modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, lets load da big boi model\n",
    "\n",
    "from numpy import  zeros, float32 as REAL\n",
    "from gensim.models import keyedvectors\n",
    "import codecs\n",
    "\n",
    "# this function was build using code excerpts from:\n",
    "# https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py\n",
    "def load_vectors_from_csv(fname,vocab_size=973265,vector_size=100):\n",
    "    print(\"Loading vectors from file:\",fname)\n",
    "    result=keyedvectors.KeyedVectors(vector_size=100)\n",
    "    result.syn0 = zeros((vocab_size, vector_size), dtype=REAL)\n",
    "    result.vector_size=vector_size\n",
    "    counts=None   \n",
    "    def add_word(word, weights):\n",
    "        word_id = len(result.vocab)\n",
    "        if word in result.vocab:\n",
    "            print(\"duplicate word '%s' in %s, ignoring all but first\", word, fname)\n",
    "            return\n",
    "        if counts is None:\n",
    "            # most common scenario: no vocab file given. just make up some bogus counts, in descending order\n",
    "            result.vocab[word] = keyedvectors.Vocab(index=word_id, count=vocab_size - word_id)\n",
    "        elif word in counts:\n",
    "            # use count from the vocab file\n",
    "            result.vocab[word] = keyedvectors.Vocab(index=word_id, count=counts[word])\n",
    "        else:\n",
    "            # vocab file given, but word is missing -- set count to None (TODO: or raise?)\n",
    "            print(\"vocabulary file is incomplete: '%s' is missing\", word)\n",
    "            result.vocab[word] = keyedvectors.Vocab(index=word_id, count=None)\n",
    "        result.syn0[word_id] = weights\n",
    "        result.index2word.append(word)   \n",
    "    file=codecs.open(fname,\"r\",\"utf-8\")\n",
    "    i=0\n",
    "    for line in file:\n",
    "        i+=1\n",
    "        if i==1: #ommit header\n",
    "            continue\n",
    "        parts=line.strip().split(\",\")\n",
    "        word,weights=parts[1],[REAL(x) for x in parts[2:]]\n",
    "        add_word(word,weights)\n",
    "        if i%100000==0:\n",
    "            print(i,\"word vectors loaded so far ...\")\n",
    "    file.close()\n",
    "    print(i-1,\"word vectors loaded!\")\n",
    "    return result\n",
    "    \n",
    "\n",
    "    \n",
    "model=load_vectors_from_csv(\"data/WORD2VEC-Twitter-Espa_ol_para_Latinoam_rica__Espa_a_y_Estados_Unidos2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creacion de modelo y visualizacion de temas con tensorboard (en construccion)\n",
    "\n",
    "\n",
    "## preprocesing twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import Counter\n",
    "from six.moves import cPickle\n",
    "import gensim.models.word2vec as w2v\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "cachedStopWords = stopwords.words(\"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generar una lista con los tweets sucios\n",
    "df = [x for x in data.msg_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion de limpieza:\n",
    "def process_raw_data(input_file):\n",
    "    url_match = \"(https?:\\/\\/[0-9a-zA-Z\\-\\_]+\\.[\\-\\_0-9a-zA-Z]+\\.?[0-9a-zA-Z\\-\\_]*\\/?.*)\"\n",
    "    name_match = \"\\@[\\_0-9a-zA-Z]+\\:?\"\n",
    "        \n",
    "    lines = input_file\n",
    "    num_lines = len(input_file)\n",
    "    ret = []\n",
    "    for text in tqdm(lines):\n",
    "        text = re.sub(url_match, u\"\", text)\n",
    "        text = re.sub(name_match, u\"\", text)\n",
    "        text = re.sub(\"\\&amp\\;?\", u\"\", text)\n",
    "        text = re.sub(\"[\\:\\.]{1,}$\", u\"\", text)\n",
    "        text = re.sub(\"^RT\\:?\", u\"\", text)\n",
    "        text = u''.join(text)\n",
    "        text = text.strip()\n",
    "        ret.append(text)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e07e40db5741e387c208c046b0dd5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=45709), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_proc = process_raw_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toquenizado\n",
    "def tokenize_sentences(sentences):\n",
    "    ret = []\n",
    "    max_s = len(sentences)\n",
    "    print(\"Got \" + str(max_s) + \" sentences.\")\n",
    "    for s in tqdm(sentences):\n",
    "        tokens = []\n",
    "        words = re.split(r'(\\s+)', s)\n",
    "        if len(words) > 0:\n",
    "            for w in words:\n",
    "                if w is not None:\n",
    "                    w = w.strip()\n",
    "                    w = w.lower()\n",
    "                    w = ''.join(c for c in w if c not in punctuation)\n",
    "                    if w.isspace() or w == \"\\n\" or w == \"\\r\":\n",
    "                        w = None\n",
    "                    if len(w) < 1:\n",
    "                        w = None\n",
    "                    if w is not None:\n",
    "                        tokens.append(w)\n",
    "        if len(tokens) > 0:\n",
    "            ret.append(tokens)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 45709 sentences.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efe5bd366504e83b1889ce642c11d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=45709), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_proc_T = tokenize_sentences(df_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limpieza de los tokens\n",
    "def clean_sentences(tokens):\n",
    "    ret = []\n",
    "    max_s = len(tokens)\n",
    "    for sentence in tqdm(tokens):\n",
    "        cleaned = []\n",
    "        for token in sentence:\n",
    "            if len(token) > 0:\n",
    "                if cachedStopWords is not None:\n",
    "                    for s in cachedStopWords:\n",
    "                        if token == s:\n",
    "                            token = None\n",
    "                if token is not None:\n",
    "                    if re.search(\"^[0-9\\.\\-\\s\\/]+$\", token):\n",
    "                        token = None\n",
    "                if token is not None:\n",
    "                    cleaned.append(token)\n",
    "        if len(cleaned) > 0:\n",
    "            ret.append(cleaned)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e60c7af9f94394abc35fbdf6a3249f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=45663), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['jo', 'també', 'però', 'diria', 'reflexiu', 'és', 'sobrer'],\n",
       " ['fuerte', 'lastima'],\n",
       " ['bro',\n",
       "  'quejo',\n",
       "  'si',\n",
       "  'parece',\n",
       "  'puta',\n",
       "  'madre',\n",
       "  'mejores',\n",
       "  'casters',\n",
       "  'lvp',\n",
       "  'solo',\n",
       "  'comentaba']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_proc_T_C = clean_sentences(df_proc_T)\n",
    "df_proc_T_C[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generacion de un vocabulario con frecuencias\n",
    "def get_word_frequencies(corpus):\n",
    "    frequencies = Counter()\n",
    "    for sentence in tqdm(corpus):\n",
    "        for word in sentence:\n",
    "            frequencies[word] += 1\n",
    "    freq = frequencies.most_common()\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c52f2508a1245ca98f3b2db329647e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=45472), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('si', 3819),\n",
       "  ('barcelona', 1914),\n",
       "  ('q', 1452),\n",
       "  ('gracias', 1398),\n",
       "  ('ser', 1339),\n",
       "  ('día', 1162),\n",
       "  ('ver', 1139),\n",
       "  ('mejor', 1114),\n",
       "  ('hoy', 1105),\n",
       "  ('bien', 1082)],\n",
       " 65357)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_proc_T_C_voc = get_word_frequencies(df_proc_T_C)\n",
    "df_proc_T_C_voc[:10],len(df_proc_T_C_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "#size: Nª of layers\n",
    "#window: Maximum distance between the current and predicted word within a sentence\n",
    "#min_count: nº of times a word repeats to be taken into account\n",
    "#sg: training algorithm. 0=cbow, 1=skip-gram\n",
    "#workers: nº of cores running. no cyton installed--> no paralelization\n",
    "\n",
    "model = gensim.models.Word2Vec(size=200, window=8, min_count=5, sg=1, workers=multiprocessing.cpu_count())\n",
    "model.build_vocab(df_proc_T_C)  # prepare the model vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10941"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nº de palabras unicas en nuestro vocabulario\n",
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(word2vec):\n",
    "    all_word_vectors_matrix = word2vec.wv.syn0\n",
    "    num_words = len(all_word_vectors_matrix)\n",
    "    vocab = word2vec.wv.vocab.keys()\n",
    "    vocab_len = len(vocab)\n",
    "    dim = word2vec.wv[vocab[0]].shape[0]\n",
    "    embedding = np.empty((num_words, dim), dtype=np.float32)\n",
    "    metadata = \"\"\n",
    "    for i, word in enumerate(vocab):\n",
    "        embedding[i] = word2vec.wv[word]\n",
    "        metadata += word + \"\\n\"\n",
    "    metadata_file = os.path.join(save_dir, \"metadata.tsv\")\n",
    "    with io.open(metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(metadata)\n",
    " \n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.InteractiveSession()\n",
    "    X = tf.Variable([0.0], name='embedding')\n",
    "    place = tf.placeholder(tf.float32, shape=embedding.shape)\n",
    "    set_x = tf.assign(X, place, validate_shape=False)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(set_x, feed_dict={place: embedding})\n",
    " \n",
    "    summary_writer = tf.summary.FileWriter(save_dir, sess.graph)\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = 'embedding:0'\n",
    "    embedding_conf.metadata_path = 'metadata.tsv'\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    " \n",
    "    save_file = os.path.join(save_dir, \"model.ckpt\")\n",
    "    print(\"Saving session...\")\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir foo:C:\\Users\\alonsoga\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fin de construccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "# unificacion de tema de tweets\n",
    "\n",
    "para cada usuario, tomamos un corpus con sus tweets. aplicamos TF/IDF a los tokens de su corpus y, con cada indicador, realizamos la media ponderada de los vectores. Obtenemos un espacio vectorial que refleja el tema del usuario.\n",
    "\n",
    "\n",
    "\n",
    "mas adelante, restaremos los espacios vectoriales entre los usuarios para saber la similitud de los usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[\"428308122\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF = (Nº de veces que una palabra aparece en un documento)/(Nº de palabras en un documento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDF = log(Nº total de documentos/Nº de documentos con el token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(corpus):\n",
    "    #nº de documentos\n",
    "    n_docs = len(corpus.keys())\n",
    "\n",
    "    #lista de tokens\n",
    "    l_tokens = []\n",
    "    \n",
    "    #dic frecuencia de cada token\n",
    "    frecuencias= {}\n",
    "\n",
    "    #dic de cada token con el nº de documentos con ese token\n",
    "    tot_frecuency = {}       \n",
    "\n",
    "    for users in corpus.keys():\n",
    "        for text in corpus[users]:\n",
    "            l_tokens.append(text)\n",
    "            if text in frecuencias:\n",
    "                frecuencias[text] +=1\n",
    "            else:\n",
    "                frecuencias[text] = 1\n",
    "\n",
    "        for word in set(l_tokens):\n",
    "            if word in tot_frecuency:\n",
    "                if word in corpus[users]:\n",
    "                    tot_frecuency[word] += 1\n",
    "            else:\n",
    "                tot_frecuency[word] = 1\n",
    "\n",
    "\n",
    "    #dic con ratio de aparicion de cada token sobre el total de tokens\n",
    "    tf = {}\n",
    "    for i in frecuencias:\n",
    "        tf[i] = frecuencias[i]/len(l_tokens)\n",
    "\n",
    "    idf = {}\n",
    "    for j in tot_frecuency.keys():\n",
    "        idf[j] = np.log(n_docs/tot_frecuency[j])\n",
    "    \n",
    "    \n",
    "    tf_idf = {}\n",
    "    for k in tf.keys():\n",
    "        tf_idf[k] = tf[k]*idf[k]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ojito cuidao que tarda en ejecutarse 10-15 minutos!\n",
    "\n",
    "corpus_tf_idf = tf_idf(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valor tf_idf de cada palabra del vocabulario\n",
    "print(list(corpus_tf_idf.keys())[:5],[corpus_tf_idf[i] for i in list(corpus_tf_idf.keys())[:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con el peso tf_idf de cada palabra, ponderamos los pesos de los vectores de las palabras de los tweets de un usuario para obtener un vector representativo del usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_corpus = {}\n",
    "\n",
    "for key in corpus.keys():\n",
    "    vector = np.array([0]*100, dtype=\"float64\")\n",
    "    contador = 0\n",
    "    for word in corpus[key]:\n",
    "        if word in model.wv.vocab:\n",
    "            vector += model.wv.word_vec(word)*corpus_tf_idf[word]\n",
    "            contador +=1\n",
    "    v_corpus[key] = vector/contador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector de tema por usuario\n",
    "v_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analizamos el coeficiente de correlacion vectorial de los usuarios entre si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(v_corpus)\n",
    "data_T = data.transpose(copy=True)\n",
    "data_T.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos una matriz de correlacion de pearson entre los usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = data.iloc[:10].corr() \n",
    "  \n",
    "f, ax = plt.subplots(figsize =(20, 20)) \n",
    "sns.heatmap(corrmat, ax = ax, cmap =\"YlGnBu\", linewidths = 0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quizas esto no tenga sentido. por qué? porque aunque sean variables continuas, que el vector de un usuario correlacione con otro solo significa que hablan de temas que se distribuyen linealmente de la misma manera. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora, la correlacion entre vectores podria indicar algo??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = data_T.corr() \n",
    "  \n",
    "f, ax = plt.subplots(figsize =(15, 15)) \n",
    "sns.heatmap(corrmat, ax = ax, cmap =\"YlGnBu\", linewidths = 0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "quizas podemos ver la cercania de los usuarios en base a la cercania de sus vectores.  \n",
    "1) **distancia euclidea** entre los vectores entre pares de usuarios: siendo 0 la igualdad y 1 la diferencia extrema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ojo que la ejecucion es hora y media!!\n",
    "euc_corpus = pd.DataFrame(columns=v_corpus.keys(), index=v_corpus.keys(), data=0, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm_notebook(v_corpus.keys()):\n",
    "    for j in v_corpus.keys():\n",
    "        if i != j and not np.isnan(v_corpus[i]).any() and not np.isnan(v_corpus[j]).any():\n",
    "            euc_corpus.loc[i][j] = distance.euclidean(v_corpus[i], v_corpus[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euc_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euc_corpus.to_csv(\"./data/euc_corpus.txt\", sep=\"&\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euc_corpus = pd.read_csv(\"./data/euc_corpus.txt\", sep=\"&\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla = list(euclidean.keys())[1:100]\n",
    "euc_corpus_20 = pd.DataFrame(index=tabla, columns=tabla, data = euc_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize =(15, 15)) \n",
    "sns.heatmap(euc_corpus_20, ax = ax, cmap =\"YlGnBu\", linewidths = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **distancia de mahalanobis**: se implementaría así, pero el tiempo innecesario y nuestro criterio, hace innecesario procesarlo. Queda a modo de ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ojo que la ejecucion es... tambien hora y media!!\n",
    "maha_corpus= pd.DataFrame(columns=v_corpus.keys(), index=v_corpus.keys(), data=0, dtype=\"float32\")\n",
    "\n",
    "for i in tqdm_notebook(v_corpus.keys()):\n",
    "    for j in v_corpus.keys():\n",
    "        if i != j and not np.isnan(v_corpus[i]).any() and not np.isnan(v_corpus[j]).any():\n",
    "            maha_corpus.loc[i][j] = dst = distance.mahalanobis(v_corpus[i], v_corpus[j], np.linalg.inv(np.cov(pd.DataFrame(v_corpus).T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/\n",
    "\n",
    "\n",
    "https://www.machinelearningplus.com/statistics/mahalanobis-distance/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion: aunque la distancia euclidea es una medida de comparacion ampliamente usada, tenemos que tener en cuenta nuestros datos para elegir la mejor. En este caso, los vectores, aunque son continuos, representan informacion categorica. Por ejemplo, en una dimension concreta de un vector, el doble de un valor no representa dos veces la unidad inicial. perro = 1, elefante =1.1\n",
    "\n",
    "\n",
    "siendo así, nos decantamos por la distancia de Mahalonobis o incluso la distancia de Bhattacharyya/Cavalli-Sforza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **Distancia Bhattacharyya / Cavalli-Sforza**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def bhattacharyya(a, b):\n",
    "\n",
    "    return -math.log(sum([np.sqrt(abs(np.multiply(u,v)), dtype=\"float32\") for u,v in zip(list(a),list(b))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ojo que la ejecucion es hora y media!!\n",
    "bha_corpus = pd.DataFrame(columns=v_corpus.keys(), index=v_corpus.keys(), data=0, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm_notebook(v_corpus.keys()):\n",
    "    for j in v_corpus.keys():\n",
    "        if i != j and not np.isnan(v_corpus[i]).any() and not np.isnan(v_corpus[j]).any():\n",
    "            bha_corpus.loc[i][j] = bhattacharyya(v_corpus[i], v_corpus[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bha_corpus.to_csv(\"./data/bha_corpus.txt\", sep=\"&\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bha_corpus = pd.read_csv(\"./data/bha_corpus.txt\", sep=\"&\", index_col=1).drop(bha_corpus.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bha_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla = list(bha_corpus.keys())[1:100]\n",
    "bha_corpus_20 = pd.DataFrame(index=tabla, columns=tabla, data = bha_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize =(15, 15)) \n",
    "sns.heatmap(bha_corpus_20, ax = ax, cmap =\"YlGnBu\", linewidths = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
