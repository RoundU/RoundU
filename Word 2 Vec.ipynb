{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://rare-technologies.com/word2vec-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lo que hacemos es utilizar una red neuronal para interrelacionar las palabras de un texto.\n",
    "Como?\n",
    "\n",
    "usando un autoencoder al que luego quitamos la ultima capa, esto nos da los vectores de cada palabra una red neuronal con n entradas (palabras unicas en el texto), m neuronas en capa intermedia, y o neuronas en la salida. el sistema intenta predecir qué palabra vendrá despues de cada palabra de entrada. siendo la palabra un vector de 1Xn.\n",
    "\n",
    "el sistema aprenderá que palabras con vectores parecidos estan inter relacionadas (pertenecen al mismo tema)\n",
    "\n",
    "al cortar la capa final, lo que nos queda es una matriz de datos con la probabilidad de aparicion de una palabra para las demas. eso es un vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import pyodbc \n",
    "import configLocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = configLocal.server\n",
    "database = configLocal.database\n",
    "username = configLocal.username\n",
    "password = configLocal.password\n",
    "cnxn = configLocal.cnxn\n",
    "cursor = cnxn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_sql_query(\"select msg_id, msg_users_id, msg_timestamp, msg_text from dbo.msg\", cnxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msg_id</th>\n",
       "      <th>msg_users_id</th>\n",
       "      <th>msg_timestamp</th>\n",
       "      <th>msg_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1123283650638942211</td>\n",
       "      <td>428308122</td>\n",
       "      <td>1556646666570</td>\n",
       "      <td>@aritzeta @bibiloni @JosepMClD @Ratafia_Party ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1123283654740983808</td>\n",
       "      <td>1669818008</td>\n",
       "      <td>1556646667548</td>\n",
       "      <td>Que fuerte!!! Y que lastima</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1123283664123703296</td>\n",
       "      <td>2295120738</td>\n",
       "      <td>1556646669785</td>\n",
       "      <td>@Helga_tito Bro no me quejo, si me parece de p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1123283666682220544</td>\n",
       "      <td>3327214991</td>\n",
       "      <td>1556646670395</td>\n",
       "      <td>@iescolar Evidente!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1123283688136032256</td>\n",
       "      <td>141861159</td>\n",
       "      <td>1556646675510</td>\n",
       "      <td>Karen me manda msj todos los días ??</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                msg_id msg_users_id  msg_timestamp  \\\n",
       "0  1123283650638942211    428308122  1556646666570   \n",
       "1  1123283654740983808   1669818008  1556646667548   \n",
       "2  1123283664123703296   2295120738  1556646669785   \n",
       "3  1123283666682220544   3327214991  1556646670395   \n",
       "4  1123283688136032256    141861159  1556646675510   \n",
       "\n",
       "                                            msg_text  \n",
       "0  @aritzeta @bibiloni @JosepMClD @Ratafia_Party ...  \n",
       "1                        Que fuerte!!! Y que lastima  \n",
       "2  @Helga_tito Bro no me quejo, si me parece de p...  \n",
       "3                              @iescolar Evidente!!!  \n",
       "4               Karen me manda msj todos los días ??  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25586, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, 5054)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = {}\n",
    "for i in range(data.shape[0]):\n",
    "    if data.iloc[i][\"msg_users_id\"] in corpus:\n",
    "        corpus[data.iloc[i][\"msg_users_id\"]].append(re.sub(r'http\\S+', '', data.iloc[i][\"msg_text\"], flags=re.MULTILINE))\n",
    "    else:\n",
    "        corpus[data.iloc[i][\"msg_users_id\"]] = [re.sub(r'http\\S+', '', data.iloc[i][\"msg_text\"], flags=re.MULTILINE)]\n",
    "type(corpus), len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@aritzeta @bibiloni @JosepMClD @Ratafia_Party @jbadia16 Jo també. Però diria que el reflexiu de ESTAR és sobrer: \\n',\n",
       " '@CUPRiudoms Sí.\\nHo he vist de casualitat. Millor un MD.',\n",
       " '@Josefina170541 @martonamg @fananwa \"Quan fou mort el combregaren\" equival a \"A buenas horas mangas verdes\".',\n",
       " '@literaturaimes @EnricGoma @martonamg \"Quan fou mort el combregaren\" equival a \"A buenas horas mangas verdes\".',\n",
       " '@mauletgroc @martonamg \"Quan fou mort el combregaren\" equival a \"A buenas horas mangas verdes\".',\n",
       " '@TrudiGonzlezMa1 @martonamg Potser sí.',\n",
       " '@victoracedoma @bibiloni @JosepMClD @Ratafia_Party @jbadia16 No, no ho és: ',\n",
       " '@Fakerragut Jo no.',\n",
       " '@Romeucarles @VogelfreiCAT Sí que ho és.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[\"428308122\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in corpus.keys():\n",
    "    corpus[i] = gensim.utils.simple_preprocess(\"\".join(corpus[i]).encode('utf-8'), deacc=True, min_len=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aritzeta',\n",
       " 'bibiloni',\n",
       " 'josepmcld',\n",
       " 'ratafia_party',\n",
       " 'jbadia',\n",
       " 'tambe',\n",
       " 'pero',\n",
       " 'diria',\n",
       " 'que',\n",
       " 'reflexiu',\n",
       " 'estar',\n",
       " 'sobrer',\n",
       " 'cupriudoms',\n",
       " 'vist',\n",
       " 'casualitat',\n",
       " 'millor',\n",
       " 'josefina',\n",
       " 'martonamg',\n",
       " 'fananwa',\n",
       " 'quan',\n",
       " 'fou',\n",
       " 'mort',\n",
       " 'combregaren',\n",
       " 'equival',\n",
       " 'buenas',\n",
       " 'horas',\n",
       " 'mangas',\n",
       " 'verdes',\n",
       " 'literaturaimes',\n",
       " 'enricgoma',\n",
       " 'martonamg',\n",
       " 'quan',\n",
       " 'fou',\n",
       " 'mort',\n",
       " 'combregaren',\n",
       " 'equival',\n",
       " 'buenas',\n",
       " 'horas',\n",
       " 'mangas',\n",
       " 'verdes',\n",
       " 'mauletgroc',\n",
       " 'martonamg',\n",
       " 'quan',\n",
       " 'fou',\n",
       " 'mort',\n",
       " 'combregaren',\n",
       " 'equival',\n",
       " 'buenas',\n",
       " 'horas',\n",
       " 'mangas',\n",
       " 'verdes',\n",
       " 'trudigonzlezma',\n",
       " 'martonamg',\n",
       " 'potser',\n",
       " 'victoracedoma',\n",
       " 'bibiloni',\n",
       " 'josepmcld',\n",
       " 'ratafia_party',\n",
       " 'jbadia',\n",
       " 'fakerragut',\n",
       " 'romeucarles',\n",
       " 'vogelfreicat',\n",
       " 'que']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[\"428308122\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prueba de una lista de listas de strings por usuario\n",
    "df = []\n",
    "for i in corpus.keys():\n",
    "    df.append(corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for i in corpus.keys():\n",
    "    for j in corpus[i]:\n",
    "        df.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5054"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of lists. Let's confirm:  <class 'list'>  of  <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(\"List of lists. Let's confirm: \", type(df), \" of \", type(df[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "#size: Nª of layers\n",
    "#window: Maximum distance between the current and predicted word within a sentence\n",
    "#min_count: nº of times a word repeats to be taken into account\n",
    "#sg: training algorithm. 0=cbow, 1=skip-gram\n",
    "#workers: nº of cores running. no cyton installed--> no paralelization\n",
    "\n",
    "model = gensim.models.Word2Vec(size=150, window=8, min_count=5, sg=1, workers=10)\n",
    "model.build_vocab(df)  # prepare the model vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(987705, 1478415)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the model\n",
    "model.train(sentences=df, total_examples=len(df), epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "model.save('./data/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tambe\n",
      "pero\n",
      "diria\n",
      "que\n",
      "estar\n",
      "vist\n",
      "millor\n",
      "quan\n",
      "mort\n",
      "buenas\n"
     ]
    }
   ],
   "source": [
    "word_vectors = model.wv\n",
    "count = 0\n",
    "for word in word_vectors.vocab:\n",
    "    if count<10:\n",
    "        print(word)\n",
    "        count += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7696"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vectors.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('juegan', 0.897331953048706), ('salvo', 0.8857386112213135), ('pudo', 0.884837806224823), ('ganado', 0.8817203044891357), ('muerte', 0.8701326847076416), ('calle', 0.869951605796814), ('siendo', 0.8695622086524963), ('tendria', 0.8689562082290649), ('error', 0.8686603307723999), ('cambio', 0.8672849535942078)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=[\"mujer\",\"rey\"],negative=[\"hombre\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, lets load da big boi model\n",
    "\n",
    "from numpy import  zeros, float32 as REAL\n",
    "from gensim.models import keyedvectors\n",
    "import codecs\n",
    "\n",
    "# this function was build using code excerpts from:\n",
    "# https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py\n",
    "def load_vectors_from_csv(fname,vocab_size=973265,vector_size=100):\n",
    "    print(\"Loading vectors from file:\",fname)\n",
    "    result=keyedvectors.KeyedVectors(vector_size=100)\n",
    "    result.syn0 = zeros((vocab_size, vector_size), dtype=REAL)\n",
    "    result.vecor_size=vector_size\n",
    "    counts=None   \n",
    "    def add_word(word, weights):\n",
    "        word_id = len(result.vocab)\n",
    "        if word in result.vocab:\n",
    "            print(\"duplicate word '%s' in %s, ignoring all but first\", word, fname)\n",
    "            return\n",
    "        if counts is None:\n",
    "            # most common scenario: no vocab file given. just make up some bogus counts, in descending order\n",
    "            result.vocab[word] = keyedvectors.Vocab(index=word_id, count=vocab_size - word_id)\n",
    "        elif word in counts:\n",
    "            # use count from the vocab file\n",
    "            result.vocab[word] = keyedvectors.Vocab(index=word_id, count=counts[word])\n",
    "        else:\n",
    "            # vocab file given, but word is missing -- set count to None (TODO: or raise?)\n",
    "            print(\"vocabulary file is incomplete: '%s' is missing\", word)\n",
    "            result.vocab[word] = keyedvectors.Vocab(index=word_id, count=None)\n",
    "        result.syn0[word_id] = weights\n",
    "        result.index2word.append(word)   \n",
    "    file=codecs.open(fname,\"r\",\"utf-8\")\n",
    "    i=0\n",
    "    for line in file:\n",
    "        i+=1\n",
    "        if i==1: #ommit header\n",
    "            continue\n",
    "        parts=line.strip().split(\",\")\n",
    "        word,weights=parts[1],[REAL(x) for x in parts[2:]]\n",
    "        add_word(word,weights)\n",
    "        if i%100000==0:\n",
    "            print(i,\"word vectors loaded so far ...\")\n",
    "    file.close()\n",
    "    print(i-1,\"word vectors loaded!\")\n",
    "    return result\n",
    "    \n",
    "\n",
    "    \n",
    "model=load_vectors_from_csv(\"data/WORD2VEC-Twitter-Espa_ol_para_Latinoam_rica__Espa_a_y_Estados_Unidos2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
